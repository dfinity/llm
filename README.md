# LLMs on the IC

This repo contains libraries and examples of how to use the [LLM canister](https://a4gq6-oaaaa-aaaab-qaa4q-cai.raw.icp0.io/?id=w36hm-eqaaa-aaaal-qr76a-cai) on the IC.

For more context about the LLM canister and its architecture, see the [forum announcement post](https://forum.dfinity.org/t/introducing-the-llm-canister-deploy-ai-agents-with-a-few-lines-of-code/41424).

## Libraries

The following libraries can be used to build AI agents on the Internet Computer with a few lines of code.

### Rust Library (`ic-llm`)

- [Explanation & Examples](rust/README.md) - Setup, usage examples, and tool documentation
- [API Documentation](https://docs.rs/ic-llm/latest/ic_llm/) - Complete API reference

### Motoko Library (`mo:llm`)

- [Explanation & Examples](motoko/README.md) - Setup, usage examples, and tool documentation
- [Package Documentation](https://mops.one/llm) - Package registry and info

### TypeScript Library (`@dfinity/llm`)

- [Explanation & Examples](typescript/README.md) - Setup, usage examples, and tool documentation
- [NPM Package](https://www.npmjs.com/package/@dfinity/llm) - Package installation and info

## Example Agents

### Quickstart Agent

This is a simple agent that simply relays whatever messages the user gives to the underlying models without any modification.
It's meant to serve as a boilerplate project for those who want to get started building agents on the IC.

A Rust and a Motoko implementation are provided in the `examples` folder.
- [Rust Quickstart Agent](examples/quickstart-agent-rust/README.md)
- [Motoko Quickstart Agent](examples/quickstart-agent-motoko/README.md)


Additionally, a live deployment of this agent can be accessed [here](https://vgjrt-uyaaa-aaaal-qsiaq-cai.icp0.io/).

![Screenshot of the quickstart agent](screenshot.png)

### ICP Lookup Agent

Showcases what it's like to build an agent that specializes in a specific task. In this case, the task is to lookup ICP prices.

A Rust and a Motoko implementation are provided in the `examples` folder.
- [Rust ICP Lookup Agent](examples/icp-lookup-agent-rust/README.md)
- [Motoko ICP Lookup Agent](examples/icp-lookup-agent-motoko/README.md)


Additionally, a live deployment of this agent can be accessed [here](https://twf3b-uqaaa-aaaal-qsiva-cai.icp0.io/).

![Screenshot of the ICP lookup agent](./examples/icp-lookup-agent-rust/screenshot.png)



## Quick Start - Local Development

### Prerequisites
- [DFX](https://internetcomputer.org/docs/building-apps/getting-started/install) installed
- [Ollama](https://ollama.com/) installed and running
- [PNPM](https://pnpm.io/) installed (for frontend examples)

### 1. Set up Ollama (Required for Local Development)

For local development, you'll need to run Ollama to process LLM requests:

```bash
# Start the Ollama server
ollama serve

# Download the required model (one-time setup)
ollama run llama3.1:8b
```

**Important**: The LLM canister deployed locally uses Ollama for processing, unlike the mainnet deployment which uses dedicated AI workers managed by DFINITY.

### 2. Configure dfx.json

Add the LLM canister to your project's `dfx.json`:

```json
{
  "canisters": {
    "llm": {
      "type": "pull",
      "id": "w36hm-eqaaa-aaaal-qr76a-cai"
    },
    "your-canister": {
      "dependencies": ["llm"],
      // ... your other canister config
    }
  }
}
```

### 3. Deploy Locally

```bash
# Start the local Internet Computer
dfx start --clean

# Pull and deploy the LLM canister dependency
dfx deps pull
dfx deps deploy

# Deploy your canisters
dfx deploy
```

### 4. Configure the LLM Backend (Optional)

The LLM canister supports two backends:

- **Ollama (Default for Local)**: Uses your local Ollama installation
- **Groq API**: Cloud-based solution requiring an API key

For Ollama (default):
```bash
dfx deps init llm --argument '(opt variant { ollama }, null)'
```

For Groq API:
```bash
dfx deps init llm --argument '(opt variant { groq = record { api_key = "YOUR_API_KEY" } }, null)'
```

## Understanding Chat Message Roles

When building chat applications, you'll work with different message types that serve specific purposes:

- **User**: Messages from the end user asking questions or making requests
- **System**: Instructions that define the AI's behavior and personality (e.g., "You are a helpful assistant")
- **Assistant**: Responses generated by the LLM
- **Tool**: Results from function calls that the LLM can make to external services

Example conversation flow:
1. **System**: "You are a helpful financial advisor"
2. **User**: "What's the current ICP price?"
3. **Assistant**: Makes a tool call to fetch ICP price
4. **Tool**: Returns the current price data
5. **Assistant**: "The current ICP price is $10.50"


